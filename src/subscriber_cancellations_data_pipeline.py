# -*- coding: utf-8 -*-
"""Subscriber_Cancellations_Data_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JbjQweX0Ut11rICBFYfncijRaPRcyml3
"""

# Below is the code to run the files locally after importing. However, the files are temporary and are deleted when the runtime disconnects or restarts.

# con = sqlite3.connect("cademycode_updated.db")
# cursor = con.cursor()
# cursor.execute("SELECT name FROM sqlite_master;")
# print(cursor.fetchall())

# This code is accessing the files from Google Drive, where the database is kept permanently

from google.colab import drive
drive.mount('/content/drive')

con = sqlite3.connect("/content/drive/MyDrive/cademycode_updated.db")

import sqlite3
import pandas as pd

# Retrieving the names of the tables in the dataset
tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", con)
tables

# Reading each table in as a dataframe
students = pd.read_sql_query("SELECT * FROM cademycode_students;", con)
courses = pd.read_sql_query("SELECT * FROM cademycode_courses;", con)
jobs = pd.read_sql_query("SELECT * FROM cademycode_student_jobs;", con)

#Getting familiar with the tables by returning the first 5 rows of each
print("Students:")
display(students.head())

print("Courses:")
display(courses.head())

print("Jobs:")
display(jobs.head())

# Custom function to gain info on the students table

def inspect_df(students):
    print("=== DataFrame Info ===")
    students.info()
    print("\n=== Summary Stats (Numeric Columns) ===")
    display(students.describe(include=['int64', 'float64']))
    print("\n=== Unique Values ===")
    for col in students.columns:
        print(f"{col}: {students[col].nunique()}")
    print("\n=== Missing Values ===")
    print(students.isnull().sum())

inspect_df(students)

# Drop rows with missing values and make a copy
students = students.dropna(subset=['job_id', 'num_course_taken', 'current_career_path_id', 'time_spent_hrs']).copy()

# Converting Date of Birth from a string to a datetime datatype
students['dob'] = pd.to_datetime(students['dob'], errors='coerce')

# Converting Number of Courses Taken, Time Spent Hours, Current Career Path Id, and Job Id from objects to numeric
students['num_course_taken'] = pd.to_numeric(students['num_course_taken'], errors='coerce')
students['time_spent_hrs'] = pd.to_numeric(students['time_spent_hrs'], errors='coerce')
students['current_career_path_id'] = pd.to_numeric(
    students['current_career_path_id'], errors='coerce'
).astype('int64')
students['job_id'] = pd.to_numeric(
    students['job_id'], errors='coerce'
).astype('int64')

# Check for duplicates by all columns
duplicate_count = students.duplicated().sum()
print(f"Total duplicate rows: {duplicate_count}")

# Remove duplicates if any
students = students.drop_duplicates().reset_index(drop=True)

import ast

# Convert string dictionaries to Python dicts and extract mailing address
def extract_address(info):
    try:
        return ast.literal_eval(info).get("mailing_address", None)
    except:
        return None

students['mailing_address'] = students['contact_info'].apply(extract_address)

# Making all column names lowercase and replacing spaces with underscores
students.columns = students.columns.str.strip().str.lower().str.replace(' ', '_')

print(students['sex'].unique())

# Null values in sex as 'N.' Replacing 'N' with null
import numpy as np

students['sex'] = students['sex'].replace('N', np.nan)
print(students['sex'].unique())

null_count = students['sex'].isnull().sum()
print(f"Number of null values in 'sex': {null_count}")

# Too many null sex values (~18% of dataset). Keeping them in as null

#Inspecting the table again after all cleaning changes
inspect_df(students)
students[['contact_info', 'mailing_address']].head()

# Using custom function to gain info on the courses table
inspect_df(courses)

# Check courses for duplicates
print(f"Duplicate rows: {courses.duplicated().sum()}")

# Check for unique 1 to 1 mapping
print(courses.groupby('career_path_id')['career_path_name'].nunique())

# Standardize column names
courses.columns = courses.columns.str.strip().str.lower().str.replace(' ', '_')

# Double check table again. All looks good.
inspect_df(courses)

# Inspect final dataframe, jobs
inspect_df(jobs)

# Check jobs for duplicates
duplicate_jobs = jobs.duplicated().sum()
print(f"Duplicate rows: {duplicate_jobs}")

# 3 duplicates found. Printing them
duplicates = jobs[jobs.duplicated(keep=False)]
print(duplicates)

# Dropping duplicates and reinspecting the dataframe
jobs = jobs.drop_duplicates().reset_index(drop=True)

inspect_df(jobs)

# Merging all dataframes into a single dataframe for export\
master_df = (
    students
    .merge(courses, left_on="current_career_path_id", right_on="career_path_id", how="left")
    .merge(jobs, on="job_id", how="left")
)

master_df.to_csv("clean_master_dataset.csv", index=False)

# Inspecting master dataframe after merge
inspect_df(master_df)

# Final verification to ensure the export file structure is correct
import pandas as pd
test_df = pd.read_csv("clean_master_dataset.csv")
print(test_df.head())